"use strict";(globalThis.webpackChunkphysical_ai_curriculum_book=globalThis.webpackChunkphysical_ai_curriculum_book||[]).push([[632],{5861(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var r=s(4848),i=s(8453);const o={sidebar_position:5},t="Python AI Agents to Robot Controllers using rclpy",a={id:"module-1-ros2/week-3-5/rclpy-bridge",title:"Python AI Agents to Robot Controllers using rclpy",description:"Introduction to rclpy",source:"@site/docs/module-1-ros2/week-3-5/rclpy-bridge.md",sourceDirName:"module-1-ros2/week-3-5",slug:"/module-1-ros2/week-3-5/rclpy-bridge",permalink:"/physical-ai-curriculum-book/docs/module-1-ros2/week-3-5/rclpy-bridge",draft:!1,unlisted:!1,editUrl:"https://github.com/Anam-Noman/physical-ai-curriculum-book/edit/main/docs/module-1-ros2/week-3-5/rclpy-bridge.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"curriculumSidebar",previous:{title:"Nodes, Topics, Services, and Actions",permalink:"/physical-ai-curriculum-book/docs/module-1-ros2/week-3-5/nodes-topics"},next:{title:"URDF for Humanoid Robot Structure",permalink:"/physical-ai-curriculum-book/docs/module-1-ros2/week-3-5/urdf-modeling"}},l={},c=[{value:"Introduction to rclpy",id:"introduction-to-rclpy",level:2},{value:"Why Python for AI Integration",id:"why-python-for-ai-integration",level:3},{value:"Setting up rclpy",id:"setting-up-rclpy",level:2},{value:"Installation and Prerequisites",id:"installation-and-prerequisites",level:3},{value:"Basic Node Structure",id:"basic-node-structure",level:3},{value:"Connecting AI Models to Robot Controllers",id:"connecting-ai-models-to-robot-controllers",level:2},{value:"Basic Publisher Example",id:"basic-publisher-example",level:3},{value:"Subscriber Example",id:"subscriber-example",level:3},{value:"Integrating with Machine Learning Libraries",id:"integrating-with-machine-learning-libraries",level:2},{value:"Using TensorFlow/Keras Models",id:"using-tensorflowkeras-models",level:3},{value:"Using PyTorch Models",id:"using-pytorch-models",level:3},{value:"Advanced Patterns: Service Integration",id:"advanced-patterns-service-integration",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Threading and Concurrency",id:"threading-and-concurrency",level:3},{value:"Quality of Service (QoS) for AI Applications",id:"quality-of-service-qos-for-ai-applications",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"python-ai-agents-to-robot-controllers-using-rclpy",children:"Python AI Agents to Robot Controllers using rclpy"}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-rclpy",children:"Introduction to rclpy"}),"\n",(0,r.jsxs)(n.p,{children:["The Robot Operating System 2 (ROS 2) Python client library, ",(0,r.jsx)(n.code,{children:"rclpy"}),", provides the interface for creating ROS 2 nodes in Python. This is particularly important for connecting AI agents written in Python to robot controllers, enabling the integration of digital AI models with physical robotic bodies."]}),"\n",(0,r.jsxs)(n.p,{children:["Python is the language of choice for many AI and machine learning applications, making ",(0,r.jsx)(n.code,{children:"rclpy"})," a crucial bridge between the AI ecosystem and robotics. This module explores how to effectively use ",(0,r.jsx)(n.code,{children:"rclpy"})," to implement the connection between Python-based AI systems and robot controllers."]}),"\n",(0,r.jsx)(n.h3,{id:"why-python-for-ai-integration",children:"Why Python for AI Integration"}),"\n",(0,r.jsx)(n.p,{children:"Python is the dominant language in AI and machine learning for several reasons:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Extensive libraries for AI/ML (TensorFlow, PyTorch, scikit-learn)"}),"\n",(0,r.jsx)(n.li,{children:"Easy prototyping and experimentation"}),"\n",(0,r.jsx)(n.li,{children:"Strong community support and documentation"}),"\n",(0,r.jsx)(n.li,{children:"Good integration with scientific computing tools"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["By using ",(0,r.jsx)(n.code,{children:"rclpy"}),", we can seamlessly integrate these AI capabilities with ROS 2's robotics infrastructure."]}),"\n",(0,r.jsx)(n.h2,{id:"setting-up-rclpy",children:"Setting up rclpy"}),"\n",(0,r.jsx)(n.h3,{id:"installation-and-prerequisites",children:"Installation and Prerequisites"}),"\n",(0,r.jsxs)(n.p,{children:["To use ",(0,r.jsx)(n.code,{children:"rclpy"}),", you need to have ROS 2 installed on your system. The library is part of the ROS 2 distribution and will be available once ROS 2 is properly installed."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Source ROS 2 environment (example for Humble Hawksbill on Ubuntu)\nsource /opt/ros/humble/setup.bash\n\n# Create and build a workspace with a Python package\nmkdir -p ~/ros2_ws/src\ncd ~/ros2_ws/src\nros2 pkg create --build-type ament_python my_ai_robot_pkg\n"})}),"\n",(0,r.jsx)(n.h3,{id:"basic-node-structure",children:"Basic Node Structure"}),"\n",(0,r.jsxs)(n.p,{children:["A typical ",(0,r.jsx)(n.code,{children:"rclpy"})," node follows this structure:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\n\nclass MyAIAgentNode(Node):\n    def __init__(self):\n        super().__init__('ai_agent_node')\n        # Initialize publishers, subscribers, services, etc.\n        self.get_logger().info('AI Agent Node initialized')\n\ndef main(args=None):\n    rclpy.init(args=args)  # Initialize rclpy\n    \n    # Create and configure the node\n    node = MyAIAgentNode()\n    \n    try:\n        # Spin the node to process callbacks\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        # Clean up\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"connecting-ai-models-to-robot-controllers",children:"Connecting AI Models to Robot Controllers"}),"\n",(0,r.jsx)(n.h3,{id:"basic-publisher-example",children:"Basic Publisher Example"}),"\n",(0,r.jsx)(n.p,{children:"Here's how to create a publisher to send commands from an AI agent to a robot controller:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nimport numpy as np  # Common AI library\n\nclass SimpleAIAgent(Node):\n    def __init__(self):\n        super().__init__('simple_ai_agent')\n        \n        # Publisher to send velocity commands to the robot\n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n        # Timer to periodically apply AI logic\n        self.timer = self.create_timer(0.1, self.ai_callback)  # 10Hz\n        \n        self.get_logger().info('Simple AI Agent initialized')\n\n    def ai_callback(self):\n        # Simple AI logic to move robot\n        cmd_msg = Twist()\n        \n        # Example AI decision (in reality, this might involve neural networks, etc.)\n        cmd_msg.linear.x = 0.5  # Move forward at 0.5 m/s\n        cmd_msg.angular.z = 0.0  # No turning\n        \n        self.cmd_publisher.publish(cmd_msg)\n        self.get_logger().info(f'Published command: linear.x={cmd_msg.linear.x}, angular.z={cmd_msg.angular.z}')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"subscriber-example",children:"Subscriber Example"}),"\n",(0,r.jsx)(n.p,{children:"To receive sensor data for AI processing:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist\n\nclass ReactiveAIAgent(Node):\n    def __init__(self):\n        super().__init__('reactive_ai_agent')\n        \n        # Publisher for robot commands\n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n        # Subscriber for sensor data\n        self.scan_subscriber = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.scan_callback,\n            10\n        )\n        \n        self.get_logger().info('Reactive AI Agent initialized')\n\n    def scan_callback(self, msg):\n        # Process sensor data to make AI decisions\n        # Find minimum distance in front of robot\n        front_scan = msg.ranges[len(msg.ranges)//2 - 90:len(msg.ranges)//2 + 90]\n        min_distance = min(front_scan)\n        \n        # Simple reactive behavior\n        cmd_msg = Twist()\n        if min_distance > 1.0:  # No obstacle nearby\n            cmd_msg.linear.x = 0.5  # Move forward\n        else:\n            cmd_msg.linear.x = 0.0  # Stop\n            cmd_msg.angular.z = 0.5  # Turn right\n            \n        self.cmd_publisher.publish(cmd_msg)\n        self.get_logger().info(f'Min distance: {min_distance:.2f}, Command: ({cmd_msg.linear.x}, {cmd_msg.angular.z})')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"integrating-with-machine-learning-libraries",children:"Integrating with Machine Learning Libraries"}),"\n",(0,r.jsx)(n.h3,{id:"using-tensorflowkeras-models",children:"Using TensorFlow/Keras Models"}),"\n",(0,r.jsx)(n.p,{children:"Here's how to run a TensorFlow model in a ROS 2 node:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport tensorflow as tf\nimport numpy as np\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\n\nclass TFAIAgent(Node):\n    def __init__(self):\n        super().__init__('tf_ai_agent')\n        \n        # Publisher for robot commands\n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n        # Subscriber for camera images\n        self.image_subscriber = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        # Bridge for converting ROS images to OpenCV format\n        self.bridge = CvBridge()\n        \n        # Load a pre-trained model\n        try:\n            self.model = tf.keras.models.load_model('/path/to/your/model.h5')\n            self.get_logger().info('Model loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load model: {e}')\n            self.model = None\n        \n        self.get_logger().info('TensorFlow AI Agent initialized')\n\n    def image_callback(self, msg):\n        if self.model is None:\n            return\n            \n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            \n            # Preprocess the image for the model\n            input_image = cv2.resize(cv_image, (224, 224)) / 255.0\n            input_image = np.expand_dims(input_image, axis=0)\n            \n            # Run inference\n            prediction = self.model.predict(input_image)\n            \n            # Use prediction to decide robot action\n            cmd_msg = Twist()\n            if prediction[0][0] > 0.5:  # If probability of \"go straight\" > 0.5\n                cmd_msg.linear.x = 0.5\n                cmd_msg.angular.z = 0.0\n            else:  # Turn\n                cmd_msg.linear.x = 0.0\n                cmd_msg.angular.z = 0.5\n                \n            self.cmd_publisher.publish(cmd_msg)\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"using-pytorch-models",children:"Using PyTorch Models"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image as PILImage\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass TorchAIAgent(Node):\n    def __init__(self):\n        super().__init__('torch_ai_agent')\n        \n        # Publisher for robot commands\n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n        # Subscriber for camera images\n        self.image_subscriber = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        self.bridge = CvBridge()\n        \n        # Load a pre-trained PyTorch model\n        try:\n            self.model = torch.load('/path/to/your/model.pth')\n            self.model.eval()  # Set to evaluation mode\n            self.get_logger().info('PyTorch model loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load PyTorch model: {e}')\n            self.model = None\n            \n        # Define image transforms\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                                std=[0.229, 0.224, 0.225])\n        ])\n\n    def image_callback(self, msg):\n        if self.model is None:\n            return\n            \n        try:\n            # Convert ROS image to OpenCV format, then to PIL for transforms\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            pil_image = PILImage.fromarray(cv_image)\n            \n            # Apply transforms\n            tensor_image = self.transform(pil_image).unsqueeze(0)\n            \n            # Run inference\n            with torch.no_grad():\n                prediction = self.model(tensor_image)\n                probabilities = torch.nn.functional.softmax(prediction[0], dim=0)\n                \n            # Use prediction to decide robot action\n            cmd_msg = Twist()\n            if torch.argmax(probabilities) == 0:  # For example: class 0 = go straight\n                cmd_msg.linear.x = 0.5\n            else:  # Turn\n                cmd_msg.angular.z = 0.5\n                \n            self.cmd_publisher.publish(cmd_msg)\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-patterns-service-integration",children:"Advanced Patterns: Service Integration"}),"\n",(0,r.jsx)(n.p,{children:"For more complex AI behaviors that require request-response patterns:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from example_interfaces.srv import Trigger\nfrom std_msgs.msg import String\n\nclass ServiceBasedAIAgent(Node):\n    def __init__(self):\n        super().__init__('service_ai_agent')\n        \n        # Publisher for robot commands\n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n        # Service server for triggering AI behaviors\n        self.ai_service = self.create_service(\n            Trigger, \n            'execute_ai_behavior', \n            self.execute_behavior_callback\n        )\n        \n        # Publisher for AI status updates\n        self.status_publisher = self.create_publisher(String, '/ai_status', 10)\n        \n        self.current_behavior = \"idle\"\n        \n    def execute_behavior_callback(self, request, response):\n        self.get_logger().info('AI behavior triggered')\n        \n        # Publish status\n        status_msg = String()\n        status_msg.data = \"executing_behavior\"\n        self.status_publisher.publish(status_msg)\n        \n        # Execute AI logic here\n        try:\n            # Example: execute a complex AI behavior\n            cmd_msg = Twist()\n            cmd_msg.linear.x = 0.3\n            cmd_msg.angular.z = 0.1\n            self.cmd_publisher.publish(cmd_msg)\n            \n            # Simulate behavior execution time\n            # In a real implementation, you might use an action server\n            # for long-running tasks\n            response.success = True\n            response.message = \"AI behavior executed successfully\"\n            \n        except Exception as e:\n            response.success = False\n            response.message = f\"Error executing behavior: {str(e)}\"\n        \n        return response\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"threading-and-concurrency",children:"Threading and Concurrency"}),"\n",(0,r.jsx)(n.p,{children:"When running AI models that may take significant time, consider using separate threads:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import threading\nfrom rclpy.qos import QoSProfile\nfrom std_msgs.msg import Bool\n\nclass ThreadedAIAgent(Node):\n    def __init__(self):\n        super().__init__('threaded_ai_agent')\n        \n        # Publisher for robot commands\n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n        # Publisher to indicate AI busy state\n        self.busy_publisher = self.create_publisher(Bool, '/ai_busy', 10)\n        \n        # Subscriber for sensor data\n        self.sensor_subscriber = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.sensor_callback,\n            10\n        )\n        \n        # Threading lock to ensure thread safety\n        self.ai_lock = threading.Lock()\n        \n    def sensor_callback(self, msg):\n        # Only start new AI processing if not already processing\n        if self.ai_lock.acquire(blocking=False):\n            # Publish busy status\n            busy_msg = Bool()\n            busy_msg.data = True\n            self.busy_publisher.publish(busy_msg)\n            \n            # Run AI processing in a separate thread\n            thread = threading.Thread(target=self.process_ai_logic, args=(msg,))\n            thread.start()\n        else:\n            self.get_logger().info('AI is busy, skipping processing')\n\n    def process_ai_logic(self, sensor_data):\n        try:\n            # Simulate AI processing time\n            # In a real implementation, this might involve neural network inference\n            # or other complex AI tasks\n            time.sleep(2)  # Simulate processing\n            \n            # Make decision based on sensor data\n            cmd_msg = Twist()\n            cmd_msg.linear.x = 0.5  # Move forward after processing\n            self.cmd_publisher.publish(cmd_msg)\n            \n        finally:\n            # Release lock and publish not-busy status\n            self.ai_lock.release()\n            busy_msg = Bool()\n            busy_msg.data = False\n            self.busy_publisher.publish(busy_msg)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"quality-of-service-qos-for-ai-applications",children:"Quality of Service (QoS) for AI Applications"}),"\n",(0,r.jsx)(n.p,{children:"Selecting appropriate QoS settings is important for robust AI-robot integration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy, HistoryPolicy\n\nclass QoSAIAgent(Node):\n    def __init__(self):\n        super().__init__('qos_ai_agent')\n        \n        # Create a QoS profile for sensor data (recent data is more important)\n        sensor_qos = QoSProfile(\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=10  # Keep only the 10 most recent messages\n        )\n        \n        # Create a QoS profile for robot commands (reliable delivery important)\n        cmd_qos = QoSProfile(\n            reliability=ReliabilityPolicy.RELIABLE,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=1\n        )\n        \n        # Publisher with specific QoS\n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', cmd_qos)\n        \n        # Subscriber with specific QoS\n        self.sensor_sub = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.scan_callback,\n            sensor_qos\n        )\n"})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"rclpy"})," library provides a powerful interface for connecting Python-based AI agents to robot controllers. By understanding and effectively using publishers, subscribers, services, and actions, you can create sophisticated systems that bridge digital AI models with physical robotic bodies."]}),"\n",(0,r.jsx)(n.p,{children:"Key takeaways:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use appropriate ROS 2 communication patterns for your AI-robot integration needs"}),"\n",(0,r.jsx)(n.li,{children:"Consider threading for computationally expensive AI operations"}),"\n",(0,r.jsx)(n.li,{children:"Select appropriate QoS settings based on your application requirements"}),"\n",(0,r.jsx)(n.li,{children:"Implement proper error handling and logging for debugging"}),"\n",(0,r.jsx)(n.li,{children:"Design your AI nodes to be modular and reusable"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These patterns form the foundation for creating embodied intelligence systems that can perform complex tasks by combining AI reasoning with physical robot action."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>t,x:()=>a});var r=s(6540);const i={},o=r.createContext(i);function t(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);