"use strict";(globalThis.webpackChunkphysical_ai_curriculum_book=globalThis.webpackChunkphysical_ai_curriculum_book||[]).push([[363],{3486(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=t(4848),o=t(8453);const r={sidebar_position:5},i="Nav2 for Humanoid Robot Path Planning",s={id:"module-3-ai-brain/week-8-10/nav2-planning",title:"Nav2 for Humanoid Robot Path Planning",description:"Introduction to Navigation in Physical AI",source:"@site/docs/module-3-ai-brain/week-8-10/nav2-planning.md",sourceDirName:"module-3-ai-brain/week-8-10",slug:"/module-3-ai-brain/week-8-10/nav2-planning",permalink:"/physical-ai-curriculum-book/docs/module-3-ai-brain/week-8-10/nav2-planning",draft:!1,unlisted:!1,editUrl:"https://github.com/Anam-Noman/physical-ai-curriculum-book/edit/main/docs/module-3-ai-brain/week-8-10/nav2-planning.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"curriculumSidebar",previous:{title:"Visual SLAM (VSLAM) and Localization",permalink:"/physical-ai-curriculum-book/docs/module-3-ai-brain/week-8-10/vslam-localization"},next:{title:"Module 4: Vision-Language-Action (VLA)",permalink:"/physical-ai-curriculum-book/docs/module-4-vla/"}},l={},c=[{value:"Introduction to Navigation in Physical AI",id:"introduction-to-navigation-in-physical-ai",level:2},{value:"Understanding Nav2 Architecture",id:"understanding-nav2-architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Key Nav2 Components",id:"key-nav2-components",level:3},{value:"Behavior Tree Navigation",id:"behavior-tree-navigation",level:3},{value:"Nav2 Configuration for Humanoid Robots",id:"nav2-configuration-for-humanoid-robots",level:2},{value:"Parameter Configuration",id:"parameter-configuration",level:3},{value:"Path Planning Algorithms",id:"path-planning-algorithms",level:2},{value:"Global Path Planners",id:"global-path-planners",level:3},{value:"A* (A-star) Planner",id:"a-a-star-planner",level:4},{value:"Dijkstra Algorithm",id:"dijkstra-algorithm",level:4},{value:"Humanoid-Specific Path Planning Considerations",id:"humanoid-specific-path-planning-considerations",level:3},{value:"Local Path Following and Control",id:"local-path-following-and-control",level:2},{value:"Controller Server Configuration",id:"controller-server-configuration",level:3},{value:"Nav2 Integration with Physical AI Systems",id:"nav2-integration-with-physical-ai-systems",level:2},{value:"Perception-Planning Integration",id:"perception-planning-integration",level:3},{value:"Advanced Navigation Strategies",id:"advanced-navigation-strategies",level:2},{value:"Multi-Robot Navigation",id:"multi-robot-navigation",level:3},{value:"Navigation Safety and Recovery",id:"navigation-safety-and-recovery",level:2},{value:"Recovery Behaviors",id:"recovery-behaviors",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Computational Efficiency",id:"computational-efficiency",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Issue 1: Path Planning Fails",id:"issue-1-path-planning-fails",level:3},{value:"Issue 2: Navigation Oscillation",id:"issue-2-navigation-oscillation",level:3},{value:"Issue 3: Local Minima Problems",id:"issue-3-local-minima-problems",level:3},{value:"Issue 4: Inconsistent Behavior",id:"issue-4-inconsistent-behavior",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"nav2-for-humanoid-robot-path-planning",children:"Nav2 for Humanoid Robot Path Planning"}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-navigation-in-physical-ai",children:"Introduction to Navigation in Physical AI"}),"\n",(0,a.jsx)(e.p,{children:"Navigation is a critical component of Physical AI systems, enabling robots to move autonomously from one location to another while avoiding obstacles and respecting environmental constraints. The Navigation2 (Nav2) project provides a comprehensive, production-ready framework for robot navigation built on ROS 2, offering sophisticated path planning and execution capabilities."}),"\n",(0,a.jsx)(e.p,{children:"For humanoid robots, navigation presents unique challenges due to their complex kinematics, balance requirements, and anthropomorphic movement patterns. This section explores how Nav2 can be configured and extended to meet the specific needs of humanoid robots."}),"\n",(0,a.jsx)(e.h2,{id:"understanding-nav2-architecture",children:"Understanding Nav2 Architecture"}),"\n",(0,a.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,a.jsx)(e.p,{children:"The Nav2 stack consists of several interconnected components that work together to provide complete navigation capabilities:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Goal \u2192 Planner Server \u2192 Path Planner \u2192 Path Post-Processing \u2192 Controller Server \u2192 Local Planner \u2192 Robot\n"})}),"\n",(0,a.jsx)(e.h3,{id:"key-nav2-components",children:"Key Nav2 Components"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Planner Server"}),": Manages global path planning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Controller Server"}),": Handles local path following and obstacle avoidance"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Recovery Server"}),": Provides recovery behaviors when navigation fails"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Lifecycle Manager"}),": Manages the state of navigation components"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"BT Navigator"}),": Uses behavior trees for navigation decision-making"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"behavior-tree-navigation",children:"Behavior Tree Navigation"}),"\n",(0,a.jsx)(e.p,{children:"Nav2 uses behavior trees to orchestrate navigation decisions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Example behavior tree configuration --\x3e\n<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <PipelineSequence name="NavigateWithReplanning">\n            <RateController hz="1.0">\n                <RecoveryNode number_of_retries="6" name="ComputeAndSmoothPath">\n                    <PipelineSequence name="ComputeAndSmooth">\n                        <RecoveryNode number_of_retries="1" name="ComputePath">\n                            <ComputePathToPose goal="{goal}" path="{path}" planner_id="GridBased"/>\n                        </RecoveryNode>\n                        <SmoothPath input_path="{path}" output_path="{path}" smoother_id="SimpleSmoother"/>\n                    </PipelineSequence>\n                    <RecoveryNode number_of_retries="1" name="SmoothPath" path="{path}"/>\n                </RecoveryNode>\n            </RateController>\n            <FollowPath path="{path}" controller_id="FollowPath"/>\n        </PipelineSequence>\n    </BehaviorTree>\n</root>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"nav2-configuration-for-humanoid-robots",children:"Nav2 Configuration for Humanoid Robots"}),"\n",(0,a.jsx)(e.h3,{id:"parameter-configuration",children:"Parameter Configuration"}),"\n",(0,a.jsx)(e.p,{children:"Humanoid robots require specific navigation parameters due to their unique kinematics and movement patterns:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:'# Example Nav2 configuration for humanoid robot\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    default_nav_through_poses_bt_xml: "navigate_w_replanning_and_recovery.xml"\n    default_nav_to_pose_bt_xml: "navigate_w_replanning_and_recovery.xml"\n\nbt_navigator_navigate_through_poses:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n\nbt_navigator_navigate_to_pose:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.001\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugins: ["progress_checker"]\n    goal_checker_plugins: ["goal_checker"]\n    controller_plugins: ["FollowPath"]\n    \n    # Humanoid-specific controller settings\n    FollowPath:\n      plugin: "nav2_mppi_controller::MPPICController"\n      debug: False\n      rate: 20\n      transform_tolerance: 0.1\n      robot_params:\n        k_velocity: 0.8\n        radius: 0.4  # Humanoid robot radius\n        footprint_model:\n          type: "polygon"\n          points: [[ 0.4,  0.3], [ 0.4, -0.3], [-0.4, -0.3], [-0.4,  0.3]]\n      costmap:\n        enabled: true\n        topic: "local_costmap/costmap_raw"\n        global_frame: "odom"\n        robot_base_frame: "base_link"\n        update_frequency: 20.0\n        width: 10\n        height: 10\n        resolution: 0.05\n      motion_model:\n        type: "Holonomic"\n        acc_lim: [2.5, 2.5, 3.2]  # Accel limits for humanoid\n        decel_lim: [-2.5, -2.5, -3.2]\n        vel_lim: [1.0, 1.0, 1.5]  # Velocity limits for humanoid (x, y, theta)\n\nlocal_costmap:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: odom\n    robot_base_frame: base_link\n    update_frequency: 5.0\n    publish_frequency: 2.0\n    width: 10\n    height: 10\n    resolution: 0.05\n    origin_x: 0.0\n    origin_y: 0.0\n    rolling_window: true\n    always_send_full_costmap: false\n    footprint: "[ [0.4, 0.3], [0.4, -0.3], [-0.4, -0.3], [-0.4, 0.3] ]"\n    footprint_padding: 0.01\n    plugins: ["voxel_layer", "inflation_layer"]\n    inflation_layer:\n      plugin: "nav2_costmap_2d::InflationLayer"\n      cost_scaling_factor: 3.0\n      inflation_radius: 0.55\n    voxel_layer:\n      plugin: "nav2_costmap_2d::VoxelLayer"\n      enabled: True\n      publish_voxel_map: False\n      origin_z: 0.0\n      z_resolution: 0.2\n      z_voxels: 8\n      max_obstacle_height: 2.0\n      mark_threshold: 0\n      observation_sources: scan\n      scan:\n        topic: /scan\n        sensor_frame: laser_link\n        max_obstacle_height: 2.0\n        clearing: True\n        marking: True\n        data_type: "LaserScan"\n        raytrace_range: 3.0\n        obstacle_range: 2.5\n        transform_tolerance: 0.2\n        inf_is_valid: True\n\nglobal_costmap:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    update_frequency: 1.0\n    static_map: True\n    rolling_window: false\n    width: 200\n    height: 200\n    resolution: 0.05\n    origin_x: 0.0\n    origin_y: 0.0\n    footprint: "[ [0.4, 0.3], [0.4, -0.3], [-0.4, -0.3], [-0.4, 0.3] ]"\n    footprint_padding: 0.01\n    plugins: ["static_layer", "obstacle_layer", "inflation_layer"]\n    obstacle_layer:\n      plugin: "nav2_costmap_2d::ObstacleLayer"\n      enabled: True\n      observation_sources: scan\n      scan:\n        topic: /scan\n        sensor_frame: laser_link\n        max_obstacle_height: 2.0\n        clearing: True\n        marking: True\n        data_type: "LaserScan"\n        raytrace_range: 3.0\n        obstacle_range: 2.5\n        transform_tolerance: 0.2\n        inf_is_valid: True\n    static_layer:\n      plugin: "nav2_costmap_2d::StaticLayer"\n      map_subscribe_transient_local: True\n    inflation_layer:\n      plugin: "nav2_costmap_2d::InflationLayer"\n      cost_scaling_factor: 3.0\n      inflation_radius: 0.55\n'})}),"\n",(0,a.jsx)(e.h2,{id:"path-planning-algorithms",children:"Path Planning Algorithms"}),"\n",(0,a.jsx)(e.h3,{id:"global-path-planners",children:"Global Path Planners"}),"\n",(0,a.jsx)(e.h4,{id:"a-a-star-planner",children:"A* (A-star) Planner"}),"\n",(0,a.jsx)(e.p,{children:"The A* algorithm finds the shortest path considering obstacles using a heuristic approach:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-cpp",children:'// Pseudo-code for A* path planning\n#include <vector>\n#include <queue>\n#include <unordered_map>\n\nstruct Node {\n    double x, y;\n    double g_cost; // Cost from start\n    double h_cost; // Heuristic cost to goal\n    double f_cost; // g_cost + h_cost\n    Node* parent;\n    \n    bool operator>(const Node& other) const {\n        return f_cost > other.f_cost;\n    }\n};\n\nstd::vector<Node> a_star_path_planner(double start_x, double start_y, \n                                     double goal_x, double goal_y,\n                                     const Costmap& costmap) {\n    // Initialize open and closed sets\n    std::priority_queue<Node, std::vector<Node>, std::greater<Node>> open_set;\n    std::unordered_map<std::string, Node> closed_set;\n    \n    // Add start node to open set\n    Node start_node = {start_x, start_y, 0.0, \n                      heuristic(start_x, start_y, goal_x, goal_y), \n                      heuristic(start_x, start_y, goal_x, goal_y), \n                      nullptr};\n    open_set.push(start_node);\n    \n    while (!open_set.empty()) {\n        Node current = open_set.top();\n        open_set.pop();\n        \n        // Check if goal is reached\n        if (distance(current.x, current.y, goal_x, goal_y) < 0.1) {\n            // Reconstruct path\n            return reconstruct_path(&current);\n        }\n        \n        // Add to closed set\n        std::string key = std::to_string(current.x) + "," + std::to_string(current.y);\n        closed_set[key] = current;\n        \n        // Explore neighbors\n        std::vector<Node> neighbors = get_neighbors(current, costmap);\n        for (auto& neighbor : neighbors) {\n            std::string neighbor_key = std::to_string(neighbor.x) + "," + std::to_string(neighbor.y);\n            \n            if (closed_set.find(neighbor_key) != closed_set.end()) {\n                continue; // Already evaluated\n            }\n            \n            // Calculate tentative g_cost\n            double tentative_g = current.g_cost + distance(current, neighbor);\n            \n            // If this path to neighbor is better than previous one\n            if (tentative_g < neighbor.g_cost) {\n                neighbor.parent = &current;\n                neighbor.g_cost = tentative_g;\n                neighbor.f_cost = neighbor.g_cost + neighbor.h_cost;\n                \n                open_set.push(neighbor);\n            }\n        }\n    }\n    \n    return {}; // No path found\n}\n'})}),"\n",(0,a.jsx)(e.h4,{id:"dijkstra-algorithm",children:"Dijkstra Algorithm"}),"\n",(0,a.jsx)(e.p,{children:"Dijkstra's algorithm finds the shortest path without using heuristics:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import heapq\nimport numpy as np\n\ndef dijkstra_path_planning(start, goal, costmap):\n    """\n    Dijkstra path planning implementation\n    """\n    # Initialize distances and previous nodes\n    distances = {}\n    previous = {}\n    unvisited = []\n    \n    # Set all distances to infinity and initialize start\n    for row in range(costmap.shape[0]):\n        for col in range(costmap.shape[1]):\n            if costmap[row, col] >= 99:  # Consider occupied cells as infinity\n                distances[(row, col)] = float(\'inf\')\n            else:\n                distances[(row, col)] = float(\'inf\')\n            previous[(row, col)] = None\n    \n    # Set start distance to 0\n    distances[start] = 0\n    heapq.heappush(unvisited, (0, start))\n    \n    while unvisited:\n        # Get node with smallest distance\n        current_distance, current_node = heapq.heappop(unvisited)\n        \n        if current_node == goal:\n            break  # Found the goal\n        \n        if current_distance > distances[current_node]:\n            continue  # Skip if already processed with a shorter distance\n        \n        # Check all neighbors\n        for dr, dc in [(-1,0), (1,0), (0,-1), (0,1), (-1,-1), (-1,1), (1,-1), (1,1)]:\n            neighbor = (current_node[0] + dr, current_node[1] + dc)\n            \n            # Check if neighbor is within bounds\n            if (0 <= neighbor[0] < costmap.shape[0] and \n                0 <= neighbor[1] < costmap.shape[1] and\n                costmap[neighbor[0], neighbor[1]] < 99):  # Not occupied\n                \n                # Calculate tentative distance\n                move_cost = 1.0 if abs(dr) + abs(dc) == 1 else 1.414  # Manhattan vs diagonal\n                tentative_distance = distances[current_node] + move_cost + costmap[neighbor[0], neighbor[1]]/100.0\n                \n                # If this path is shorter than any previous path to neighbor\n                if tentative_distance < distances[neighbor]:\n                    distances[neighbor] = tentative_distance\n                    previous[neighbor] = current_node\n                    heapq.heappush(unvisited, (tentative_distance, neighbor))\n    \n    # Reconstruct path\n    path = []\n    current = goal\n    while current != start:\n        if previous[current] is None:\n            return []  # No path found\n        path.append(current)\n        current = previous[current]\n    \n    path.append(start)\n    path.reverse()\n    \n    return path\n'})}),"\n",(0,a.jsx)(e.h3,{id:"humanoid-specific-path-planning-considerations",children:"Humanoid-Specific Path Planning Considerations"}),"\n",(0,a.jsx)(e.p,{children:"Humanoid robots require special consideration in path planning:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Balance Constraints"}),": The path must maintain the robot's balance and stability"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Step Constraints"}),": The robot can only step on certain surfaces"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Footstep Planning"}),": Generate valid footstep sequences"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Kinematic Constraints"}),": Respect joint limits and reachability"]}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class HumanoidPathPlanner:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.step_length_limit = 0.3  # Maximum step length\n        self.step_height_limit = 0.15  # Maximum step height difference\n        self.support_polygon = self.calculate_support_polygon()\n    \n    def plan_holonomic_path(self, start_pose, goal_pose, costmap):\n        \"\"\"\n        Plan path considering humanoid-specific constraints\n        \"\"\"\n        # Convert to grid coordinates\n        start = self.world_to_grid(start_pose)\n        goal = self.world_to_grid(goal_pose)\n        \n        # Use A* for initial path\n        raw_path = self.a_star_search(start, goal, costmap)\n        \n        # Post-process for humanoid constraints\n        humanoid_path = self.post_process_for_humanoid(raw_path)\n        \n        # Generate footstep plan\n        footsteps = self.generate_footsteps(humanoid_path)\n        \n        return humanoid_path, footsteps\n    \n    def post_process_for_humanoid(self, raw_path):\n        \"\"\"\n        Modify path to respect humanoid step constraints\n        \"\"\"\n        processed_path = [raw_path[0]]\n        \n        i = 0\n        while i < len(raw_path) - 1:\n            j = i + 1\n            \n            # Find the furthest point reachable in one step\n            while (j < len(raw_path) and \n                   self.calculate_distance(raw_path[i], raw_path[j]) < self.step_length_limit):\n                j += 1\n            \n            # Add the furthest reachable point\n            if j > i + 1:\n                processed_path.append(raw_path[j-1])\n                i = j - 1\n            else:\n                processed_path.append(raw_path[i+1])\n                i += 1\n        \n        return processed_path\n    \n    def generate_footsteps(self, path):\n        \"\"\"\n        Generate footstep sequence for the humanoid robot\n        \"\"\"\n        footsteps = []\n        left_support = True  # Start with left foot support\n        \n        for i in range(len(path) - 1):\n            step = {\n                'position': path[i+1],\n                'orientation': self.calculate_step_orientation(path[i], path[i+1]),\n                'step_type': 'left' if left_support else 'right',\n                'timing': self.calculate_step_timing(path[i], path[i+1])\n            }\n            footsteps.append(step)\n            left_support = not left_support  # Alternate feet\n        \n        return footsteps\n\n# Example of footstep planning for humanoid robots\ndef plan_footsteps_for_humanoid(start_pos, goal_pos):\n    \"\"\"\n    Generate footstep plan for humanoid robot navigation\n    \"\"\"\n    # Calculate step sequence\n    dx = goal_pos[0] - start_pos[0]\n    dy = goal_pos[1] - start_pos[1]\n    distance = np.sqrt(dx**2 + dy**2)\n    \n    # Estimate number of steps needed\n    num_steps = max(int(distance / 0.3), 1)  # Assuming 0.3m step length\n    \n    footsteps = []\n    for i in range(1, num_steps + 1):\n        # Calculate intermediate position\n        step_x = start_pos[0] + (dx / num_steps) * i\n        step_y = start_pos[1] + (dy / num_steps) * i\n        \n        # Add footstep with alternating feet\n        footstep = {\n            'x': step_x,\n            'y': step_y,\n            'theta': np.arctan2(dy, dx),  # Heading direction\n            'foot': 'left' if i % 2 == 0 else 'right',\n            'step_count': i\n        }\n        footsteps.append(footstep)\n    \n    return footsteps\n"})}),"\n",(0,a.jsx)(e.h2,{id:"local-path-following-and-control",children:"Local Path Following and Control"}),"\n",(0,a.jsx)(e.h3,{id:"controller-server-configuration",children:"Controller Server Configuration"}),"\n",(0,a.jsx)(e.p,{children:"The controller server manages local path following and dynamic obstacle avoidance:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Example controller implementation\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Path\nfrom geometry_msgs.msg import Twist\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport math\n\nclass HumanoidController(Node):\n    def __init__(self):\n        super().__init__('humanoid_controller')\n        \n        # Parameters\n        self.declare_parameters(\n            namespace='',\n            parameters=[\n                ('goal_dist_threshold', 0.2),\n                ('goal_yaw_threshold', 0.1),\n                ('max_linear_vel', 0.5),\n                ('max_angular_vel', 1.0),\n                ('min_linear_vel', 0.1),\n                ('min_angular_vel', 0.1),\n                ('control_freq', 10.0),\n                ('kp_linear', 1.0),\n                ('kp_angular', 2.0)\n            ]\n        )\n        \n        # Get parameters\n        self.goal_dist_threshold = self.get_parameter('goal_dist_threshold').value\n        self.goal_yaw_threshold = self.get_parameter('goal_yaw_threshold').value\n        self.max_linear_vel = self.get_parameter('max_linear_vel').value\n        self.max_angular_vel = self.get_parameter('max_angular_vel').value\n        self.min_linear_vel = self.get_parameter('min_linear_vel').value\n        self.min_angular_vel = self.get_parameter('min_angular_vel').value\n        self.control_freq = self.get_parameter('control_freq').value\n        self.kp_linear = self.get_parameter('kp_linear').value\n        self.kp_angular = self.get_parameter('kp_angular').value\n        \n        # Subscribers and publishers\n        self.path_sub = self.create_subscription(\n            Path, 'global_plan', self.path_callback, 10\n        )\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        \n        # TF2 setup\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n        \n        # Path following variables\n        self.current_path = None\n        self.current_waypoint_idx = 0\n        \n        # Control timer\n        self.control_timer = self.create_timer(1.0/self.control_freq, self.control_loop)\n        \n        self.get_logger().info('Humanoid Controller initialized')\n\n    def path_callback(self, msg):\n        \"\"\"Receive and store path to follow\"\"\"\n        self.current_path = msg\n        self.current_waypoint_idx = 0\n        self.get_logger().info(f'Received path with {len(msg.poses)} waypoints')\n\n    def control_loop(self):\n        \"\"\"Main control loop for path following\"\"\"\n        if self.current_path is None or len(self.current_path.poses) == 0:\n            return\n        \n        # Get current robot pose\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                'map', 'base_link', rclpy.time.Time()\n            )\n            current_x = transform.transform.translation.x\n            current_y = transform.transform.translation.y\n            current_yaw = self.quaternion_to_yaw(transform.transform.rotation)\n        except TransformException as e:\n            self.get_logger().error(f'Could not get transform: {e}')\n            return\n        \n        # Get current target waypoint\n        if self.current_waypoint_idx >= len(self.current_path.poses):\n            # Reached goal\n            self.stop_robot()\n            return\n        \n        target_pose = self.current_path.poses[self.current_waypoint_idx]\n        target_x = target_pose.pose.position.x\n        target_y = target_pose.pose.position.y\n        \n        # Calculate distance and angle to target\n        dist_to_target = math.sqrt((target_x - current_x)**2 + (target_y - current_y)**2)\n        \n        target_yaw = math.atan2(target_y - current_y, target_x - current_x)\n        angle_to_target = self.normalize_angle(target_yaw - current_yaw)\n        \n        # Check if reached current waypoint\n        if dist_to_target < self.goal_dist_threshold:\n            self.current_waypoint_idx += 1\n            if self.current_waypoint_idx >= len(self.current_path.poses):\n                # Reached end of path\n                self.stop_robot()\n                return\n            \n            # Get next waypoint\n            target_pose = self.current_path.poses[self.current_waypoint_idx]\n            target_x = target_pose.pose.position.x\n            target_y = target_pose.pose.position.y\n            dist_to_target = math.sqrt((target_x - current_x)**2 + (target_y - current_y)**2)\n            target_yaw = math.atan2(target_y - current_y, target_x - current_x)\n            angle_to_target = self.normalize_angle(target_yaw - current_yaw)\n        \n        # Calculate velocities - with humanoid-specific constraints\n        linear_vel = min(self.kp_linear * dist_to_target, self.max_linear_vel)\n        angular_vel = min(self.kp_angular * abs(angle_to_target), self.max_angular_vel)\n        \n        # Adjust sign of angular velocity\n        if angle_to_target < 0:\n            angular_vel = -angular_vel\n        \n        # Create and publish command\n        cmd_msg = Twist()\n        cmd_msg.linear.x = max(linear_vel, self.min_linear_vel)  # Ensure minimum velocity for stability\n        cmd_msg.angular.z = angular_vel\n        \n        self.cmd_vel_pub.publish(cmd_msg)\n        \n        # Log control information\n        self.get_logger().debug(\n            f'Control: linear={cmd_msg.linear.x:.2f}, angular={cmd_msg.angular.z:.2f}, '\n            f'distance={dist_to_target:.2f}, angle={math.degrees(angle_to_target):.1f}\xb0'\n        )\n\n    def stop_robot(self):\n        \"\"\"Stop the robot movement\"\"\"\n        cmd_msg = Twist()\n        cmd_msg.linear.x = 0.0\n        cmd_msg.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd_msg)\n        self.get_logger().info('Robot stopped - reached goal')\n\n    def quaternion_to_yaw(self, quat):\n        \"\"\"Convert quaternion to yaw angle\"\"\"\n        siny_cosp = 2 * (quat.w * quat.z + quat.x * quat.y)\n        cosy_cosp = 1 - 2 * (quat.y * quat.y + quat.z * quat.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def normalize_angle(self, angle):\n        \"\"\"Normalize angle to [-pi, pi] range\"\"\"\n        while angle > math.pi:\n            angle -= 2 * math.pi\n        while angle < -math.pi:\n            angle += 2 * math.pi\n        return angle\n"})}),"\n",(0,a.jsx)(e.h2,{id:"nav2-integration-with-physical-ai-systems",children:"Nav2 Integration with Physical AI Systems"}),"\n",(0,a.jsx)(e.h3,{id:"perception-planning-integration",children:"Perception-Planning Integration"}),"\n",(0,a.jsx)(e.p,{children:"Integrating perception with navigation planning:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Odometry, Path\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Bool\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nimport tf2_ros\n\nclass PerceptionAwareNav(Node):\n    def __init__(self):\n        super().__init__(\'perception_aware_nav\')\n        \n        # Create clients for navigation services\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n        \n        # Subscribers for perception data\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \'scan\', self.lidar_callback, 10\n        )\n        self.odom_sub = self.create_subscription(\n            Odometry, \'odom\', self.odom_callback, 10\n        )\n        \n        # Publishers for visualization\n        self.obstacle_pub = self.create_publisher(MarkerArray, \'detected_obstacles\', 10)\n        \n        # Perception processing\n        self.obstacle_threshold = 1.0  # Minimum distance to consider obstacle\n        self.robot_radius = 0.4  # Robot radius for collision checking\n        \n        # TF buffer for coordinate transformations\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n        \n        self.get_logger().info(\'Perception-aware navigation node initialized\')\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data for obstacle detection"""\n        # Convert laser scan to obstacle markers\n        obstacles = self.extract_obstacles_from_scan(msg)\n        \n        # Publish for visualization\n        obstacle_markers = self.create_obstacle_markers(obstacles)\n        self.obstacle_pub.publish(obstacle_markers)\n        \n        # Update local costmap with detected obstacles\n        self.update_costmap_with_obstacles(obstacles)\n\n    def extract_obstacles_from_scan(self, scan_msg):\n        """Extract obstacle positions from laser scan"""\n        obstacles = []\n        angle = scan_msg.angle_min\n        \n        for range_val in scan_msg.ranges:\n            if not np.isnan(range_val) and range_val < self.obstacle_threshold:\n                # Convert polar to Cartesian coordinates\n                x = range_val * np.cos(angle)\n                y = range_val * np.sin(angle)\n                \n                # Transform to map frame\n                obstacle_map_frame = self.transform_to_map_frame(x, y, 0.0)\n                \n                if obstacle_map_frame is not None:\n                    obstacles.append(obstacle_map_frame)\n            \n            angle += scan_msg.angle_increment\n        \n        return obstacles\n\n    def navigate_with_obstacle_avoidance(self, goal_pose):\n        """Navigate to goal with dynamic obstacle avoidance"""\n        # Send initial navigation goal\n        goal = NavigateToPose.Goal()\n        goal.pose = goal_pose\n        \n        # Wait for result with feedback\n        future = self.nav_client.send_goal_async(goal)\n        future.add_done_callback(self.navigation_result_callback)\n        \n        # Continuously monitor environment for new obstacles\n        self.obstacle_monitor_timer = self.create_timer(\n            0.5,  # Check for obstacles every 0.5 seconds\n            self.check_environment\n        )\n\n    def check_environment(self):\n        """Check environment for new obstacles that might require replanning"""\n        # Implement logic to detect if current path is blocked\n        if self.is_path_blocked():\n            # Cancel current navigation and replan\n            self.nav_client.cancel_goal_async(self.current_goal_handle)\n            self.replan_path()\n\n    def is_path_blocked(self):\n        """Check if current planned path is blocked by obstacles"""\n        # Implementation would check current path against latest sensor data\n        # This is a simplified version\n        return False\n\n    def replan_path(self):\n        """Replan path considering new obstacles"""\n        # Get current robot pose\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                \'map\', \'base_link\', rclpy.time.Time()\n            )\n            current_pose = PoseStamped()\n            current_pose.pose.position.x = transform.transform.translation.x\n            current_pose.pose.position.y = transform.transform.translation.y\n            current_pose.pose.position.z = transform.transform.translation.z\n            current_pose.pose.orientation = transform.transform.rotation\n        except tf2_ros.TransformException:\n            self.get_logger().error(\'Could not get robot pose\')\n            return\n        \n        # Get goal pose (maintaining the same goal)\n        # In practice, you\'d store the original goal\n        # For this example, assume we have a stored goal\n        if hasattr(self, \'stored_goal\'):\n            self.navigate_with_obstacle_avoidance(self.stored_goal)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-navigation-strategies",children:"Advanced Navigation Strategies"}),"\n",(0,a.jsx)(e.h3,{id:"multi-robot-navigation",children:"Multi-Robot Navigation"}),"\n",(0,a.jsx)(e.p,{children:"For coordinating multiple humanoid robots:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nfrom std_msgs.msg import String\nimport json\n\nclass MultiRobotNav(Node):\n    def __init__(self):\n        super().__init__(\'multi_robot_nav\')\n        \n        # Communication channel for coordination\n        self.coordination_pub = self.create_publisher(\n            String, \'navigation_coordination\', 10\n        )\n        self.coordination_sub = self.create_subscription(\n            String, \'navigation_coordination\', self.coordination_callback, 10\n        )\n        \n        # Robot ID for coordination\n        self.robot_id = self.declare_parameter(\'robot_id\', \'robot_0\').value\n        \n        # Active robots and their goals\n        self.active_robots = {}\n        \n        self.get_logger().info(f\'Multi-robot navigation node {self.robot_id} initialized\')\n\n    def coordination_callback(self, msg):\n        """Handle coordination messages from other robots"""\n        try:\n            coord_data = json.loads(msg.data)\n            sender_id = coord_data[\'robot_id\']\n            \n            if sender_id != self.robot_id:\n                self.active_robots[sender_id] = coord_data[\'current_goal\']\n        except Exception as e:\n            self.get_logger().error(f\'Error parsing coordination message: {e}\')\n\n    def request_path_reservation(self, path):\n        """Request reservation of path to avoid conflicts with other robots"""\n        # Calculate path segments\n        path_segments = self.discretize_path(path, resolution=0.5)\n        \n        # Check for conflicts with other robots\n        conflicts = self.check_path_conflicts(path_segments)\n        \n        if not conflicts:\n            # No conflicts, proceed with navigation\n            return True\n        else:\n            # Plan alternative route or wait\n            return self.handle_conflicts(conflicts)\n\n    def discretize_path(self, path, resolution=0.5):\n        """Discretize path into segments for conflict checking"""\n        segments = []\n        \n        for i in range(len(path.poses) - 1):\n            start = path.poses[i].pose.position\n            end = path.poses[i+1].pose.position\n            \n            # Calculate number of intermediate points\n            dist = ((end.x - start.x)**2 + (end.y - start.y)**2)**0.5\n            num_points = int(dist / resolution)\n            \n            for j in range(num_points):\n                t = j / num_points\n                x = start.x + t * (end.x - start.x)\n                y = start.y + t * (end.y - start.y)\n                \n                segments.append((x, y, i))  # Include original segment index\n        \n        return segments\n\n    def check_path_conflicts(self, path_segments):\n        """Check if planned path conflicts with other robots"""\n        conflicts = []\n        \n        for other_robot_id, other_goal in self.active_robots.items():\n            # Check if paths intersect in space and time\n            if self.paths_conflict(path_segments, other_goal):\n                conflicts.append(other_robot_id)\n        \n        return conflicts\n\n    def paths_conflict(self, path1_segments, path2_goal):\n        """Check if two paths conflict"""\n        # Implementation would consider timing and spatial overlap\n        # Simplified for this example\n        return False\n\n    def handle_conflicts(self, conflicts):\n        """Handle path conflicts with other robots"""\n        # Implement negotiation strategy\n        # For now, just return False to indicate replanning needed\n        return False\n'})}),"\n",(0,a.jsx)(e.h2,{id:"navigation-safety-and-recovery",children:"Navigation Safety and Recovery"}),"\n",(0,a.jsx)(e.h3,{id:"recovery-behaviors",children:"Recovery Behaviors"}),"\n",(0,a.jsx)(e.p,{children:"Nav2 includes various recovery behaviors to handle navigation failures:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example of custom recovery behavior\nfrom nav2_behavior_tree import Condition\n\nclass ClearPathRecovery(Condition):\n    def __init__(self, name, condition_params):\n        super().__init__(name)\n        self.recovery_count = 0\n        self.max_recoveries = 3\n\n    def condition(self):\n        # Check if clear path is available\n        local_costmap = self.context.get_local_costmap()\n        \n        # Check immediate area around robot\n        robot_pos = self.context.get_robot_position()\n        \n        # Check if path is clear within robot\'s footprint\n        if self.is_path_clear(robot_pos, local_costmap):\n            self.recovery_count = 0  # Reset on success\n            return True\n        else:\n            if self.recovery_count < self.max_recoveries:\n                # Attempt to clear path by moving slightly\n                self.execute_recovery()\n                self.recovery_count += 1\n                return False  # Still attempting recovery\n            else:\n                # Too many failures, give up\n                return False\n\n    def is_path_clear(self, robot_pos, costmap):\n        """Check if immediate path is clear"""\n        # Implementation would check costmap values around robot\n        return True  # Simplified for example\n\n    def execute_recovery(self):\n        """Execute recovery action"""\n        # Move robot slightly to clear immediate obstacles\n        recovery_cmd = Twist()\n        recovery_cmd.linear.x = 0.1  # Move forward slowly\n        recovery_cmd.angular.z = 0.0\n        # Publish command to robot\n'})}),"\n",(0,a.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"computational-efficiency",children:"Computational Efficiency"}),"\n",(0,a.jsx)(e.p,{children:"Optimizing navigation for real-time performance:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class EfficientPathPlanner:\n    def __init__(self):\n        self.path_cache = {}\n        self.last_poses = {}\n        \n        # Use spatial data structures for efficient lookup\n        from scipy.spatial import cKDTree\n        self.cached_paths_kdtree = None\n    \n    def get_cached_path(self, start, goal, tolerance=0.5):\n        """Get path from cache if start/end points are similar"""\n        cache_key = (round(start[0], 1), round(start[1], 1), \n                     round(goal[0], 1), round(goal[1], 1))\n        \n        if cache_key in self.path_cache:\n            return self.path_cache[cache_key]\n        \n        return None\n    \n    def plan_path_with_optimization(self, start, goal, costmap):\n        """Plan path with optimization techniques"""\n        # First, check if path is available in cache\n        cached_path = self.get_cached_path(start, goal)\n        if cached_path:\n            return cached_path\n        \n        # Simplified path planning with optimization\n        # Use grid-based approach for efficiency\n        grid_resolution = 0.5  # meters per grid cell\n        \n        # Convert world coordinates to grid coordinates\n        start_grid = (int(start[0] / grid_resolution), int(start[1] / grid_resolution))\n        goal_grid = (int(goal[0] / grid_resolution), int(goal[1] / grid_resolution))\n        \n        # Use A* for path planning\n        path = self.a_star_path_planner(start_grid, goal_grid, costmap)\n        \n        # Convert back to world coordinates\n        world_path = [(x * grid_resolution, y * grid_resolution) for x, y in path]\n        \n        # Cache the result\n        cache_key = (round(start[0], 1), round(start[1], 1), \n                     round(goal[0], 1), round(goal[1], 1))\n        self.path_cache[cache_key] = world_path\n        \n        return world_path\n'})}),"\n",(0,a.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,a.jsx)(e.h3,{id:"issue-1-path-planning-fails",children:"Issue 1: Path Planning Fails"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": Robot cannot find path to goal"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),": Check costmap settings, increase inflation radius, validate map quality"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"issue-2-navigation-oscillation",children:"Issue 2: Navigation Oscillation"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": Robot oscillates around obstacles"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),": Adjust controller parameters, increase minimum velocities, tune PID gains"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"issue-3-local-minima-problems",children:"Issue 3: Local Minima Problems"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": Robot gets stuck in local obstacles"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),": Implement better recovery behaviors, use more sophisticated planners"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"issue-4-inconsistent-behavior",children:"Issue 4: Inconsistent Behavior"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": Navigation works sometimes but not others"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),": Check sensor reliability, validate TF trees, ensure proper synchronization"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Parameter Tuning"}),": Carefully tune Nav2 parameters for humanoid robot characteristics"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Map Quality"}),": Ensure high-quality maps for reliable localization"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor Validation"}),": Verify sensor data quality before navigation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Measures"}),": Implement appropriate safety checks and fallbacks"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Testing"}),": Extensively test navigation in various environments and scenarios"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Monitoring"}),": Implement comprehensive monitoring and logging for debugging"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Nav2 provides a robust framework for humanoid robot navigation, integrating path planning, obstacle avoidance, and dynamic replanning capabilities. By properly configuring Nav2 for humanoid-specific characteristics and constraints, robots can effectively navigate complex environments while maintaining balance and stability."}),"\n",(0,a.jsx)(e.p,{children:"The integration of perception with navigation planning creates intelligent systems that can adapt to dynamic environments and make informed decisions about movement. These capabilities are essential for Physical AI systems that must operate in real-world environments, bridging the gap between digital AI models and physical robotic bodies."}),"\n",(0,a.jsx)(e.p,{children:"Understanding the Nav2 architecture and its components enables the development of sophisticated navigation behaviors that allow humanoid robots to operate autonomously and safely in diverse environments."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>i,x:()=>s});var a=t(6540);const o={},r=a.createContext(o);function i(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:i(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);